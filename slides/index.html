<!--
slidedeck: A modification of the Google IO 2012 HTML5 slide template
URL: https://github.com/rmcgibbo/slidedeck

Based on https://github.com/francescolaffi/elastic-google-io-slides, and
ultimately:

Google IO 2012 HTML5 Slide Template
Authors: Eric Bidelman <ebidel@gmail.com>
         Luke Mahe <lukem@google.com>
URL: https://code.google.com/p/io-2012-slides
-->
<!DOCTYPE html>
<html>
<head>
  <title> OCR on Indus Seals</title>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <!--<meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0">-->
  <!--<meta name="viewport" content="width=device-width, initial-scale=1.0">-->
  <!--This one seems to work all the time, but really small on ipad-->
  <!--<meta name="viewport" content="initial-scale=0.4">-->
  <meta name="apple-mobile-web-app-capable" content="yes">
   <link rel="shortcut icon" href=" figures/favicon.ico"/> 
  <link rel="stylesheet" media="all" href="theme/css/default.css">
  <link rel="stylesheet" media="all" href="theme/css/custom.css">
  <link rel="stylesheet" media="only screen and (max-device-width: 480px)" href="theme/css/phone.css">
  <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->
  <script data-main="js/slides", src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.14/require.min.js"></script>


  <!-- MathJax support  -->
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    showProcessingMessages: false,
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    TeX: {
      extensions: ["color.js"]
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
  </script>
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <div style="display:hidden">
  \[
    \definecolor{data}{RGB}{18,110,213}
    \definecolor{unknown}{RGB}{217,86,16}
    \definecolor{learned}{RGB}{175,114,176}
  \]
  </div>

</head>

<body style="opacity: 0">

<slides class="layout-widescreen">
<slide class="title-slide segue nobackground">
  <hgroup class="auto-fadein">

    <h1> OCR on Indus Seals</h1>
    <h2> Satish Palaniappan, SSNCE</h2>
    <p> Under the Guidance of,<br/> Dr. Ronojoy Adhikari, Institute of Mathematical Sciences, Chennai.</p>
  </hgroup>
</slide>


<slide  >
  
    <hgroup>
      <h2>Overview</h2>
      <h3></h3>
    </hgroup>
    <article ><p><strong>The Problem:</strong></p>
<ul class="build">
<li>Statement</li>
<li>Expected Output</li>
<li>Architectural Design</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Overview</h2>
      <h3></h3>
    </hgroup>
    <article ><p><strong>Methodology:</strong></p>
<ul class="build">
<li>Formulating the dataset</li>
<li>Region proposal<ul class="build">
<li>Selective Search</li>
<li>Fine tuning and Region grouping</li>
</ul>
</li>
<li>Text region filtering (using Convolutional Neural Networks)<ul class="build">
<li>Text - no text classifier</li>
<li>Filtering and Trimming region proposals</li>
</ul>
</li>
<li>Symbol segmentation</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Overview</h2>
      <h3></h3>
    </hgroup>
    <article ><p><strong>The Results:</strong></p>
<ul class="build">
<li>Evaluating the pipeline</li>
</ul>
<p><strong>Future Prospects:</strong></p>
<ul class="build">
<li>Symbol Identification<ul class="build">
<li>Jar sign experimentation</li>
<li>Empirical Analysis of Pipeline’s Performance</li>
</ul>
</li>
<li>Image augmentation and preprocessing</li>
</ul></article>
 
</slide>

<slide class="segue dark nobackground" >
  
    <!-- <aside class="gdbar"><img src="images/google_developers_icon_128.png"></aside> -->
    <hgroup class="auto-fadein">
      <h2>The Problem</h2>
      <h3></h3>
    </hgroup>
  
</slide>

<slide  >
  
    <hgroup>
      <h2>Problem Statement</h2>
      <h3></h3>
    </hgroup>
    <article ><p style="text-align:justify">To automatically locate text patches/regions, segment individual symbols/characters from those regions and also identify each symbol/character belonging to the Indus Script, given images of Indus seals from archaeological sites, using image processing and machine learning techniques.</p></article>
 
</slide>

<slide class="img-top-center" >
  
    <hgroup>
      <h2>Expected Output</h2>
      <h3></h3>
    </hgroup>
    <article ><p style="text-align:justify">Given Indus seal images, we intend to extract the text sequences as shown below (mapping to the corresponding symbol numbers in the M77 Corpus),
</p>

<p><img src=figures/expected_output.png /></p></article>
 
</slide>

<slide class="img-top-center" >
  
    <hgroup>
      <h2>Architectural design of the proposed system</h2>
      <h3></h3>
    </hgroup>
    <article ><p><img src=figures/arch_design.png /></p></article>
 
</slide>

<slide class="img-top-center" >
  
    <hgroup>
      <h2>Overview of the system</h2>
      <h3></h3>
    </hgroup>
    <article ><p><img src=figures/overview.png /></p></article>
 
</slide>

<slide class="segue dark nobackground" >
  
    <!-- <aside class="gdbar"><img src="images/google_developers_icon_128.png"></aside> -->
    <hgroup class="auto-fadein">
      <h2>Methodology</h2>
      <h3></h3>
    </hgroup>
  
</slide>

<slide class="img-top-center" >
  
    <hgroup>
      <h2>Architectural design of the proposed system</h2>
      <h3></h3>
    </hgroup>
    <article ><p><img src=figures/arch_design1.png /></p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Formulating the Dataset: Initial Steps</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>A script written in Python was used to access the Google Custom Search API to get Indus Seal Images from the Google image search engine</li>
<li>A maximum of 100 images per search term was retrieved</li>
<li>
<p><strong>Search terms used:</strong> <span style="font-size:70%">["indus seals", "harappan seals", "harappan pashupati seal", "harappan unicorn seal", "indus inscriptions", "harappan seals wikipedia", "indus seals and inscriptions", "indus seal stones", "seal impressions indus valley civilization", "indus valley tiger seals", "indus valley seals yoga", "indus valley seals for kids" ]
</span></p>
</li>
<li>
<p>Removed noisy images manually and got 350 useful images out of the 1000 images downloaded, this dataset is refered to as the, “crawled dataset”.</p>
</li>
</ul></article>
 
</slide>

<slide class="img-top-center" >
  
    <hgroup>
      <h2>A snapshot of the crawled dataset</h2>
      <h3></h3>
    </hgroup>
    <article ><p><img vspace="100px" height="300px" src=figures/crawled_data.png /></p></article>
 
</slide>

<slide class="img-top-center" >
  
    <hgroup>
      <h2>Text/No Text Dataset</h2>
      <h3></h3>
    </hgroup>
    <article ><p style="text-align:justify; font-size: smaller"> This was formulated by running the selective search algorithm for region proposal (discussed in the upcoming slides) over the 350 seal images from the “crawled dataset” and then manually grouping the resulting 872 regions into those containing, not containing and partly containing the Indus text, to be used by the “Text/No Text Classifier” (discussed in upcoming slides).</p>

<p><img height="300px" src=figures/tnot_data.png /></p></article>
 
</slide>

<slide class="img-top-center" >
  
    <hgroup>
      <h2>Architectural design of the proposed system</h2>
      <h3></h3>
    </hgroup>
    <article ><p><img src=figures/arch_design2.png /></p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Region Proposal</h2>
      <h3>Selective Search Algorithm</h3>
    </hgroup>
    <article ><ul>
<li>Given an image it proposes various regions of interests that is more likely to have an object within it</li>
<li>It is the fastest algorithm to compute and it combines the advantages of exhaustive search and segmentation</li>
<li>It performs hierarchical grouping of region proposals based on colour, texture, size, fill, etc. to propose the best ROIs</li>
</ul></article>
 
</slide>

<slide class="img-top-center" >
  
    <hgroup>
      <h2>Selective Search - Working</h2>
      <h3></h3>
    </hgroup>
    <article ><p><img height="400" src=figures/sel_search.png /></p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Selective Search - Fine Tuning</h2>
      <h3></h3>
    </hgroup>
    <article ><p>In order to improve the region proposals to suit the purpose of identifying text regions in seal images, a greedy grid search approach over 4 parameters was performed to identify the best combination for a 512x512 image</p>
<ul>
<li><strong>Scale</strong> - Higher the value larger the clusters in felzenszwalb segmentation (350, 450, 500)</li>
<li><strong>Sigma</strong> - Width of Gaussian kernel for felzenszwalb segmentation (0.8)</li>
<li><strong>Min Size</strong> - Minimum component size for felzenszwalb segmentation (30, 60, 120)</li>
<li><strong>Min Area</strong> - Minimum area of a region proposed (2000)</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2></h2>
      <h3></h3>
    </hgroup>
    <article ><p>Once these parameters were fine tuned the regions proposed were relevant enough but were really high in number. Also they were mostly approximations and generalizations of each other.</p>
<p><img height="400" src=figures/sel_search1.png /></p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Selective Search - Customised Grouping</h2>
      <h3></h3>
    </hgroup>
    <article ><p style="text-align:justify; font-size: smaller"> In order to reduce the number of regions proposed and to increase the quality of the region proposals the following hierarchical grouping methods were devised, (These were applied on images scaled to 512x512 or 256x256 or original size): </p>

<ul>
<li>
<p><p style="text-align:justify; font-size: smaller"> <strong>Merge Concentric Proposals:</strong> Most of the proposals were focusing on the same object with just small variations in the position and area being covered. Such proposals were merged together and replaced by the mean rectangle of all the concentric proposals </p></p>
</li>
<li>
<p><p style="text-align:justify; font-size: smaller"> <strong>Contained Boxes Removal:</strong> Some other proposals were subsets of overall text regions, some fraction of each symbols within a text region was also proposed along with the full text region. So, all of these subsets were removed and only the overall proposals were retained </p></p>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2></h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>
<p><p style="text-align:justify; font-size: smaller"> <strong>Draw Super Box:</strong> Other proposals were overlapping each other such that a single symbol or text region was proposed as two different overlapping regions. The percentage overlap of such proposals was calculated and thresholded at 20 percent, all those pairs of regions having more than 20 percent overlap were replaced by a single minimal super box that bounded both the proposals
</p></p>
</li>
<li>
<p><p style="text-align:justify; font-size: smaller"> <strong>Draw Extended Super Box:</strong> The regions in hand now were continuous subtext regions in the seal, arranged along the horizontal or vertical axes of the image. As all the subtext regions along the same axis belonged to a piece of text normally, all these were replaced by a single horizontal/vertical super box
</p></p>
</li>
</ul></article>
 
</slide>

<slide class="img-top-center" >
  
    <hgroup>
      <h2>Region Proposal - Output</h2>
      <h3></h3>
    </hgroup>
    <article ><p><img height="330px" src=figures/sel_search2.png /></p></article>
 
</slide>

<slide class="img-top-center" >
  
    <hgroup>
      <h2>Architectural design of the proposed system</h2>
      <h3></h3>
    </hgroup>
    <article ><p><img src=figures/arch_design3.png /></p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Text Region Filtering using CNN</h2>
      <h3>Why Text region filtering?</h3>
    </hgroup>
    <article ><p>From the results of our region proposal mechanism discussed above, it is clear that the final region proposals have both text and nontext regions being proposed, sometimes a single region proposal might even have both text and non text portions in it. In order to filter these proposals, a CNN image classifier is going to be trained, which will facilitate the region filtering and trimming</p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Convolutional Neural Network</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>The concept of CNN is inspired from the actual working of the human eye and how neurons processes images</li>
<li>The main strength of using a CNN is that it works as a feature extractor with deep learnt features as well as a classifier</li>
<li>The Caffe deep learning framework developed by “Yangqing Jia” under <a href="http://bvlc.eecs.berkeley.edu/">Berkeley Vision</a>, was used to build the CNNs</li>
</ul></article>
 
</slide>

<slide class="img-top-center" >
  
    <hgroup>
      <h2>A sample CNN Architecture</h2>
      <h3></h3>
    </hgroup>
    <article ><p><img src=figures/cnn1.png /></p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Why Deep Features?</h2>
      <h3></h3>
    </hgroup>
    <article ><p style="text-align:justify"> Generally, the way images are vectorized into features has always been hand crafted, but now with increasing problem complexity these deep learnt features are capable of adapting themselves on focusing about, what to look in the images given the requirements, instead of hand-crafting it. With just minimal or no pre-processing, a hierarchy of features can be learnt and with less effort.</p></article>
 
</slide>

<slide class="img-top-center" >
  
    <hgroup>
      <h2>Features samples learnt at various stages of CNN</h2>
      <h3></h3>
    </hgroup>
    <article ><p><img src=figures/cnn2.png /></p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>CNN Architecture</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>The CNNs consist of a variety of layers like<ul>
<li>Convolution, Pooling, LRN, ReLU, Dropout, Full Connected, SoftMax Classifier</li>
<li>each of these layers have a number of parameters that can be configured.</li>
</ul>
</li>
<li>This blueprint that describes the hierarchy of the layers and the parameter configurations is referred to as the CNN Architecture.</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Classifiers trained using CNN (1)</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li><strong>Jar Symbol Binary Classifier</strong><ul>
<li><strong>Dataset</strong> -  The Jar Sign Dataset</li>
<li><strong>Architecture</strong> -  The pannous/caffe-OCR CNN OCR Architecture</li>
<li><strong>Purpose</strong> - To detect the presence of the most frequently encountered Indus symbol, the Jar, from the given Indus seal images</li>
<li><strong>Note:</strong> This model has no application in the OCR pipeline being discussed currently and was just an experiment</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Classifiers trained using CNN (2)</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li><strong>Text/No Text Classifier</strong><ul>
<li><strong>Dataset</strong> - The Text/No Text Dataset</li>
<li><strong>Architecture</strong> - The GoogLeNet CNN Architecture</li>
<li><strong>Purpose</strong> - To classify regions proposed by the Selective Search algorithm in the previous step, into 3 main classes namely - Text, No Text and Both, thus facilitating region filtering and trimming</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide class="img-top-center" >
  
    <hgroup>
      <h2>Architectural design of the proposed system</h2>
      <h3></h3>
    </hgroup>
    <article ><p><img src=figures/arch_design4.png /></p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Text/No Text Classifier</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>From the results of our region proposal, it is clear that the final region proposals have both text and nontext regions being proposed, sometimes a single region proposal might even have text and non text portions in it. In order to filter these proposals, a CNN image classifier was built by fine tuning the <a href="http://arxiv.org/abs/1409.4842">GoogLeNet CNN Architecture </a> trained on ImageNet images to suit our needs. There were three classes considered, Text, No Text and Both. The “Text/No text Dataset” was used for this purpose.</li>
<li><strong>Dataset size:</strong> Text - 232, No-Text - 543, Both - 97 images respectively, 70:30 stratified split for train and test was used</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Text/No Text Classifier - GoogLeNet Architecture</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>GoogLeNet is a 22 layers deep network</li>
<li>It is one of the most efficient and powerful implementations of CNN</li>
<li>It was mainly designed for object classification and detection problems</li>
<li>It has 9 Inception modules (Network within a network)</li>
</ul></article>
 
</slide>

<slide class="img-top-center" >
  
    <hgroup>
      <h2>GoogLeNet - Inception Module</h2>
      <h3></h3>
    </hgroup>
    <article ><p><img height="400px" src=figures/cnn_inception.png /></p></article>
 
</slide>

<slide class="img-top-center" >
  
    <hgroup>
      <h2>GoogLeNet - Architecture</h2>
      <h3></h3>
    </hgroup>
    <article ><p><img src=figures/cnn_arch.png /></p></article>
 
</slide>

<slide class="img-top-center" >
  
    <hgroup>
      <h2>Architectural design of the proposed system</h2>
      <h3></h3>
    </hgroup>
    <article ><p><img src=figures/arch_design5.png /></p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Using pre-learnt GoogLeNet weights</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>GoogLeNet was designed for extremely large datasets and hence fails to deliver high accuracy for datasets as small as 872 images</li>
<li>A concept called, <i>Transfer Learning</i> comes into picture, where the weights and filters learnt by the convolutional neural networks on some other dataset could be fine tuned and transfer learnt to suit our dataset</li>
<li>In order to achieve transfer learning in our case, the GoogLeNet was initialized with the pre-trained weights and filters that were trained on the <a href="http://www.image-net.org/">ImageNet</a> images</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Fine Tuning the Network</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>The initial layers of the network had the rich low level filters of the ImageNet images initialized.</li>
<li>The learning rate for these layers was set to 0 and as the network progressed into its depth the already available base learning rates were doubled.</li>
<li>Links to the SoftMax classifier was initialized to random weights with 3 outputs in our case.</li>
<li>SGD solver’s parameters were modified such that the learning rate was 0.001 with a step size of 3200 and 16000 iterations.</li>
</ul></article>
 
</slide>

<slide class="img-top-center" >
  
    <hgroup>
      <h2>GoogLeNet - Solver Configuration</h2>
      <h3></h3>
    </hgroup>
    <article ><p><img height="400px" src=figures/cnn3.png /></p></article>
 
</slide>

<slide class="img-top-center" >
  
    <hgroup>
      <h2>Text Filtering  - Results Obtained</h2>
      <h3></h3>
    </hgroup>
    <article ><p>This gave a model with a recall of 93.76%, for text/no text classification of the ROIs
<img height="300px" src=figures/table1.png /></p></article>
 
</slide>

<slide class="img-top-center" >
  
    <hgroup>
      <h2>Architectural design of the proposed system</h2>
      <h3></h3>
    </hgroup>
    <article ><p><img src=figures/arch_design6.png /></p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Filtering and trimming region proposals</h2>
      <h3></h3>
    </hgroup>
    <article ><p>After getting the region proposals from selective search and their labels (Text, No Text and Both) from the Text/No Text Classifier, the following two methods were applied to generate much more accurate and crisp text regions.</p>
<ul>
<li><strong>Draw TextBox:</strong> In some pairs of region proposals, where a Text region and a Both region, were overlapping, In order to get the whole text regions, the overlapping boxes were merged to a single text box</li>
<li><strong>Trim TextBox:</strong> In some pairs of region proposals, where a Text Box region and a NoText, were overlapping, In order to get the trimmed text regions, the overlapping boxes were clipped to a single text box</li>
</ul></article>
 
</slide>

<slide class="img-top-center" >
  
    <hgroup>
      <h2>Trimming Region Proposals - Results Obtained</h2>
      <h3></h3>
    </hgroup>
    <article ><p><img src=figures/trim.png /></p></article>
 
</slide>

<slide class="img-top-center" >
  
    <hgroup>
      <h2>Architectural design of the proposed system</h2>
      <h3></h3>
    </hgroup>
    <article ><p><img src=figures/arch_design7.png /></p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Symbol Segmentation (1)</h2>
      <h3></h3>
    </hgroup>
    <article ><p>Once we have the text regions, we need to segment out the characters/symbols, for that purpose the selective search algorithm was not effective, So, a customized algorithm that involved the the following steps was used to get the individual symbols out of the text regions,</p>
<ul>
<li>Gray scale the image</li>
<li>Black and White Thresholding [by Otsu] to get a discrete binary image (easy to segment)</li>
<li>Gaussian Blur to remove noise</li>
</ul>
<p style="text-align: right">continued...</p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Symbol Segmentation (2)</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>SciPy's find_object() method to get the connected subregions in the image</li>
<li>Region grouping mechanisms discussed previously applied with slight modifications</li>
<li>Incorporate these regions over the original image</li>
</ul></article>
 
</slide>

<slide class="img-top-center" >
  
    <hgroup>
      <h2>Symbol Segmentation - Results Obtained</h2>
      <h3></h3>
    </hgroup>
    <article ><p><img src=figures/sym1.png /></p></article>
 
</slide>

<slide class="segue dark nobackground" >
  
    <!-- <aside class="gdbar"><img src="images/google_developers_icon_128.png"></aside> -->
    <hgroup class="auto-fadein">
      <h2>The Results</h2>
      <h3></h3>
    </hgroup>
  
</slide>

<slide  >
  
    <hgroup>
      <h2>Evaluating the Pipeline (1)</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>In order to evaluate the performance of the text localization and segmentation pipeline discussed above, 25 unique seal images were taken from Google and was passed through the Indus OCR pipeline.</li>
<li><strong>Results:</strong><ul>
<li><strong>Text Localization:</strong> Text regions from 23 out of 25 images were extracted successfully</li>
<li>However, In 5 of those 23 images the proposed text regions missed one symbol 92% Accuracy (Approximate)</li>
</ul>
</li>
</ul>
<p style="text-align: right">continued...</p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Evaluating the Pipeline (2)</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li><strong>Symbol extraction:</strong> Out of those 23 images, symbols were successfully extracted from 14 images.<ul>
<li>Out of the 9 failed images, 5 images were of low quality and were blurred</li>
<li>The remaining 4 images of those 9, failed due to physical damages in the seals</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide class="img-top-center" >
  
    <hgroup>
      <h2>Results - Perfectly Working</h2>
      <h3></h3>
    </hgroup>
    <article ><p><img height="230px" src=figures/result1.png /></p></article>
 
</slide>

<slide class="img-top-center" >
  
    <hgroup>
      <h2>Results - Failed Cases</h2>
      <h3></h3>
    </hgroup>
    <article ><p><img height="230px" src=figures/result2.png /></p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Empirical Analysis of Pipeline’s Performance (1)</h2>
      <h3></h3>
    </hgroup>
    <article ><p>The symbol extraction pipeline has some drawbacks in terms of performance, it fails to perfectly extract symbol regions from the indus seal images, due to performance dropout at various stages of the pipeline, let us see where and why are there performance degrades,</p>
<ul>
<li><strong>Text Localization</strong><ul>
<li><strong>Problem:</strong>  The text region proposed, misses out one symbol or fails to localize the text patch</li>
<li><strong>Reason:</strong> Blurred images, Complex seal structure, Contrasting / Bad lighted pictures</li>
</ul>
</li>
</ul>
<p style="text-align: right">continued...</p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Empirical Analysis of Pipeline’s Performance (2)</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li><strong>Symbol Extraction</strong><ul>
<li>Problem: The symbol extraction module failed to extract complete symbol regions</li>
<li>Reason: Physical damages in seals , Low quality and blurred images</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide class="segue dark nobackground" >
  
    <!-- <aside class="gdbar"><img src="images/google_developers_icon_128.png"></aside> -->
    <hgroup class="auto-fadein">
      <h2>Future Prospects</h2>
      <h3></h3>
    </hgroup>
  
</slide>

<slide class="img-top-center" >
  
    <hgroup>
      <h2>Architectural design of the proposed system</h2>
      <h3></h3>
    </hgroup>
    <article ><p><img src=figures/arch_design8.png /></p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Symbol Identification</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>After perfecting the symbol segmentation module of the above discussed Indus OCR pipeline, we need to identify each symbol and classify them into one of the 417 classes of Indus symbols according to the Mahadevan Corpus(M77)</li>
<li>For training such a CNN classifier, we need to generate a more robust dataset augmenting the available base dataset with noise and gather more seal images labeled with corresponding text sequences</li>
<li>As a prior experimentation, we wanted to build a classifier that was capable of identifying, the presence of the most frequently spotted Indus symbol, the JAR sign, in the Indus Seal images, called the Jar sign experimentation.</li>
</ul></article>
 
</slide>

<slide class="img-top-center" >
  
    <hgroup>
      <h2>Jar sign experimentation - Dataset Formulation</h2>
      <h3></h3>
    </hgroup>
    <article ><p style="text-align: justify"> <b>The Jar Sign Dataset:</b> This was formulated by manually classifying the 350 seal images from the “crawled dataset”, into those containing and not containing the Jar sign, to be used by the “Jar sign binary classifier”. </p>

<p><img src=figures/jarsign.png /></p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Jar sign experimentation - Training the Classifier</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>The Jar Symbol Binary Classifier was built to detect the presence of Jar sign in the Indus seal image</li>
<li>For this purpose a CNN architecture named IndusNet, inspired from <a href="https://github.com/pannous/caffe-ocr">“pannous/caffe-OCR”</a> was built. The skeleton is shown below,<ul>
<li><p style="text-align: justify; font-size: 65%">DATA[32x32x3](scaled down by 256) -&gt; Convolution[5x5 Kernel, Stride 1, 20 Outputs] -&gt; Convolution[5x5 Kernel, Stride 1, 50 Outputs] -&gt; Dropout (to prevent over-fitting) -&gt; Fully Connected [500 Outputs] -&gt; ReLU (Non linearity)-&gt; Fully Connected [2 Outputs](2 Classes) -&gt; SoftMax Classifier </p></li>
</ul>
</li>
<li>The dataset was formulated by applying a 70:30 stratified split over the “Jar sign dataset”</li>
<li>The model built had an accuracy of 93.4% after 1000 iterations</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Data Augmentation and Pre-processing</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li><strong>Data Augmentation:</strong> An image data augmentation tool was developed to achieve the same. The basic code was inspired from the online image data augmenter of <a href="http://keras.io/">“Keras“</a>, which included,<ul>
<li><p style="text-align: justify">Horizontal and vertical flip, Rotation, scaling, translation, Shear, Swirl, Blur, Contrast, Brightness (TO DO)</p></li>
</ul>
</li>
<li><strong>Pre-processing:</strong>  Some of the other common image pre-processing methods to be used are feature-wise mean-zero and standard normalization, sample-wise mean-zero and standard normalization</li>
</ul></article>
 
</slide>


<slide class="thank-you-slide segue nobackground">
  <!-- <aside class="gdbar right"><img src="images/google_developers_icon_128.png"></aside> -->
  <article class="flexbox vleft auto-fadein">
    <h2> Thanks everyone!</h2>
    <p></p>
  </article>
  <p data-config-contact class="auto-fadein"> <span>www</span> <a href="https://in.linkedin.com/in/satishpalaniappan">Satish Palaniappan</a><br/> <span>github</span> <a href="http://github.com/tpsatish95">tpsatish95</a></p>
  </p>
</slide>

<slide class="backdrop"></slide>

</slides>

<script>
var _gaq = _gaq || [];
_gaq.push(['_setAccount', 'UA-XXXXXXXX-1']);
_gaq.push(['_trackPageview']);

(function() {
  var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
  ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
  var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
})();
</script>

<!--[if IE]>
  <script src="http://ajax.googleapis.com/ajax/libs/chrome-frame/1/CFInstall.min.js"></script>
  <script>CFInstall.check({mode: 'overlay'});</script>
<![endif]-->
</body>
</html>